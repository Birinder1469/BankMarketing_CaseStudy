{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Casestudy_RIS_Modelling.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxsnDbWQOZfK",
        "outputId": "fa04c578-0ebe-4c49-94b3-6f7a275079f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "# Mount the Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load packages \n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Pre processing packages \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Modelling packages \n",
        "import xgboost as xgb\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# Evaluation packages \n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import  RandomizedSearchCV, GridSearchCV, cross_val_score\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
        "file_location = '/content/drive/MyDrive/SW_CodingPractice/RIS/'\n",
        "os.chdir(file_location)"
      ],
      "metadata": {
        "id": "OD8Kfn2gOhXj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('bank-additional/bank-additional-full.csv', sep = ';')"
      ],
      "metadata": {
        "id": "F1-KlfPcOhZ1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation "
      ],
      "metadata": {
        "id": "C0ZKQAnROqAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a great deal of understanding about the dataset. For building the model we need to do some data preparation so we can feed it to the model. From the previous analysis I know there are duplicate entries. Let me remove those first. "
      ],
      "metadata": {
        "id": "svIN24zkOsG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop_duplicates().reset_index(drop=True)"
      ],
      "metadata": {
        "id": "j8XSEUSQOheA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several categorical features in our dataset. There are couple of ways to handle them. <br>\n",
        "1. One Hot encoding : If the variables have no order (Nominal variables) then assign a vector of size equal to the number of categories and then binary [0,1] based on the presence or absence of that category for each row/data sample. The issue with this approach is that it introduces sparseness. It becomes challenging when we have very high cardinality.\n",
        "2. Numerical Encoding: If there is an inherent order to the variable(Ordinal variables) then assign a numerical value. For instance T shirt size Small, Medium and Large.\n",
        "3. Binary Encoding: We can first convert the categories into numerical using an ordinal encoder. Then transform the numerical value in the binary number. This binary number is then split into different columns. This way we do not get a sparse matrix like we do in One hot encoding. This technique is generally preferred when the cardinality is high.\n",
        "4. Bin or combine the categories if that is feasible for the dataset and then convert the bins to numbers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5efqS6u6PIVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will also remove the feature `duration` as discussed in the EDA."
      ],
      "metadata": {
        "id": "pI3XQFZHguHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(columns = 'duration')"
      ],
      "metadata": {
        "id": "yd2VoD9fgwvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know there were 96% clients who were not previously contacted. The column pdays has 96% 999 values and rest are small values in magnitude. I will convert this column into two categories the Clients who were not contacted previosuly i.e pdays = 999 as Category A and the rest as Category B."
      ],
      "metadata": {
        "id": "maM1ix3j5A1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.pdays = np.where(data.pdays==999,'A','B')\n",
        "data.pdays = data.pdays.astype('object')"
      ],
      "metadata": {
        "id": "O5AsT2-q9kGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me rename the y column to target and assign numerical values instead of 'yes' and 'no'."
      ],
      "metadata": {
        "id": "GmUuqAwEgxVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.rename(columns = {'y':'target'})\n",
        "map_values = {'yes':1,'no':0}\n",
        "data.target = data.target.replace(map_values)"
      ],
      "metadata": {
        "id": "NrJuWzcagmMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape[0]"
      ],
      "metadata": {
        "id": "BWP6FEpYOhgm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9e1253-72d2-4057-905d-974b7a610371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41176"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For modelling purpose, My intention is the keep some portion of dataset for final evaluation. It behaves as unseen data, I will call this Test dataset. Besides that I need data to train and validate my results. I will split the data into 3 parts. This practice reduces the risk of Data Leakage.\n",
        "This technique of data splitting has been stressed a lot by Prof Andrew Ng. Youtube video from the course on Machine Learning I attended: https://www.youtube.com/watch?v=MyBSkmUeIEs"
      ],
      "metadata": {
        "id": "vHxU2gwRSZJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, I know our dataset is imbalanced I need to make sure there is sufficient representation of minority class in each of the dataset. "
      ],
      "metadata": {
        "id": "E3YFNsawTCYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data in Train and Test(HoldOut)\n",
        "train_validation, test_data = train_test_split(data, test_size = 0.10, random_state = 123)\n",
        "\n",
        "# Next I split the Train data further into Train and Validation(To check in case we are overfitting)\n",
        "train_data, validation_data = train_test_split(train_validation, test_size = 0.15, random_state = 123)\n",
        "\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "validation_data= validation_data.reset_index(drop=True)\n",
        "test_data = test_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "3oa2F4NNOhjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the porportion of minority class in each dataset?"
      ],
      "metadata": {
        "id": "GO6r_GWh5jPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.target.value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmVvha1oxjEl",
        "outputId": "69f74783-0f25-463e-8df5-669e30b2a420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.886758\n",
              "1    0.113242\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data.target.value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DDh6vgUxnFD",
        "outputId": "5ff401b5-ff1b-4116-8547-d76a9a2265b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.890808\n",
              "1    0.109192\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.target.value_counts(normalize = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNGYwCs-xnH6",
        "outputId": "daf4d06d-36d1-45b4-a556-7060f4f66860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.887081\n",
              "1    0.112919\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Sample size for training dataset: {0}'.format(train_data.shape[0]))\n",
        "print('Sample size for validation dataset: {0}'.format(validation_data.shape[0]))\n",
        "print('Sample size for test dataset: {0}'.format(test_data.shape[0]))"
      ],
      "metadata": {
        "id": "Zr6iqARgOhpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cba9f6d-e8e0-4b83-8b8c-f32bb3987ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size for training dataset: 31499\n",
            "Sample size for validation dataset: 5559\n",
            "Sample size for test dataset: 4118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great, we have proportion of both the categories in all the 3 datasets."
      ],
      "metadata": {
        "id": "JpaDYeOMpCTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One Hot Encoding "
      ],
      "metadata": {
        "id": "CtACw2v3pQJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only the columns with object category need to be encoded."
      ],
      "metadata": {
        "id": "bYOyS-f_pZ8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us use One hot encoding from sklearn. I am implementing this step by step for each of the datasets. This is not the most optimized way but I am interested in checking the outputs of each step. If I have to write a script I will use functions instead of doing the following tasks step by step."
      ],
      "metadata": {
        "id": "bBQezeAjpaAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtype_groups = train_data.drop(columns='target').columns.to_series().groupby(train_data.drop(columns='target').dtypes).groups\n",
        "feature_types = {k.name: list(v) for k, v in dtype_groups.items()}"
      ],
      "metadata": {
        "id": "I6k4gtSQVYyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_types['object']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv9lGtYjVY1F",
        "outputId": "84f35025-8137-4a6f-cd0d-4aafec6ebd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['job',\n",
              " 'marital',\n",
              " 'education',\n",
              " 'default',\n",
              " 'housing',\n",
              " 'loan',\n",
              " 'contact',\n",
              " 'month',\n",
              " 'day_of_week',\n",
              " 'pdays',\n",
              " 'poutcome']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training data encoding "
      ],
      "metadata": {
        "id": "mwKMyDD_ppIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_categorical = train_data.loc[:,feature_types['object']]\n",
        "train_data_numeric = train_data.loc[:,feature_types['float64'] + feature_types['int64']]"
      ],
      "metadata": {
        "id": "-xo2oqDsVY41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype=int)"
      ],
      "metadata": {
        "id": "Jf_LxCMrVZGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe.fit(train_data_categorical)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHKUunTYVZIg",
        "outputId": "fa45a7fd-cb5c-4332-f8b4-17c431c2ba08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneHotEncoder(dtype=<class 'int'>, handle_unknown='ignore', sparse=False)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_array_train = ohe.transform(train_data_categorical)\n",
        "ohe_array_train_df = pd.DataFrame(ohe_array_train, columns = ohe.get_feature_names_out(feature_types['object']))\n",
        "train_data_encoded = pd.concat([train_data_numeric,ohe_array_train_df], axis =1)"
      ],
      "metadata": {
        "id": "m7kV3f0RVZRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation data encoding"
      ],
      "metadata": {
        "id": "jGDXpdZEaw4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data_categorical = validation_data.loc[:,feature_types['object']]\n",
        "validation_data_numeric = validation_data.loc[:,feature_types['float64'] + feature_types['int64']]"
      ],
      "metadata": {
        "id": "Guo0GjQppvVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_array_valid = ohe.transform(validation_data_categorical)\n",
        "ohe_array_valid_df = pd.DataFrame(ohe_array_valid, columns = ohe.get_feature_names_out(feature_types['object']))\n",
        "valid_data_encoded = pd.concat([validation_data_numeric,ohe_array_valid_df], axis =1)"
      ],
      "metadata": {
        "id": "yNIUiRqUaDUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test data encoding "
      ],
      "metadata": {
        "id": "lvhf8PMua8d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_categorical = test_data.loc[:,feature_types['object']]\n",
        "test_data_numeric = test_data.loc[:,feature_types['float64'] + feature_types['int64']]"
      ],
      "metadata": {
        "id": "CFxlkRv2pwqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe_array_test = ohe.transform(test_data_categorical)\n",
        "ohe_array_test_df = pd.DataFrame(ohe_array_test, columns = ohe.get_feature_names_out(feature_types['object']))\n",
        "test_data_encoded = pd.concat([test_data_numeric,ohe_array_test_df], axis =1)"
      ],
      "metadata": {
        "id": "8nD092bzaDZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([train_data_encoded,train_data.target], axis =1).to_csv('train_data_encoded.csv', index = False)\n",
        "pd.concat([valid_data_encoded,validation_data.target], axis =1).to_csv('validation_data_encoded.csv', index = False)\n",
        "pd.concat([test_data_encoded,test_data.target], axis =1).to_csv('test_data_encoded.csv', index = False)"
      ],
      "metadata": {
        "id": "wrUsVZVeaD2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling "
      ],
      "metadata": {
        "id": "kvlbDMhIbzIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the column values are at different scales. For any algorithm based on calculating the distance we need the features to be on same scale also for gradient descent the convergence will be faster. Let us use the standard scaler from sklearn to scale the features."
      ],
      "metadata": {
        "id": "PAl-un7GqHxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit on the training dataset\n",
        "scaler.fit(train_data_encoded)\n",
        "\n",
        "# Transform the training and validation datasets\n",
        "X_train_scaled = scaler.transform(train_data_encoded)\n",
        "X_validation_scaled = scaler.transform(valid_data_encoded)"
      ],
      "metadata": {
        "id": "pPzxBj2aaD5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_data.target\n",
        "y_validation = validation_data.target\n"
      ],
      "metadata": {
        "id": "3XRXlICacBmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling "
      ],
      "metadata": {
        "id": "WchBVxArrNfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point we have the dataset ready to be put into any ML predictive algorithm. But its important to make a choice of which algorithms to go forward with. My intention is to first use the baseline models and later dig into hyperparameter tuning to find the best set of hyperparameters of the model. I will not use the Test dataset till the last step.\n",
        "\n",
        "We have a binary classification problem where the target class is highly imbalanced. Following models can be used for prediction:\n",
        "\n",
        "- SVM\n",
        "- Naive Bayes\n",
        "- Logistic Regression\n",
        "- Tree based algorithm (Decision Tree)\n",
        "- Ensemble techniques (Bagging - Random forest) and (Boosting - XGBoost Ada boost etc.)\n",
        "- Neural networks (MLP) <br>\n",
        "There are pros and cons of each of the algorithm, for instance if we are intersted in the interpretation of the results Logistic regression is a better choice but it will be able to fit a linear hyperplane only. SVM and Neural networks can model more complex non linear hyperplanes but they are not so interpretable."
      ],
      "metadata": {
        "id": "yUre3ZPKrRY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression\n"
      ],
      "metadata": {
        "id": "RdXn8-ej3_fG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model \n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnhxrkQMrWnw",
        "outputId": "82edcf62-f528-4e91-abb6-14926558bfa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on Training and Validation datasets\n",
        "y_predict_train = lr.predict_proba(X_train_scaled)\n",
        "y_predict_valid = lr.predict_proba(X_validation_scaled)"
      ],
      "metadata": {
        "id": "u9Im_DXLrWrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation with AUC \n",
        "train_score_lr = roc_auc_score(y_train, y_predict_train[:,1])\n",
        "valid_score_lr = roc_auc_score(y_validation, y_predict_valid[:,1])\n",
        "print(\"Training ROC-AUC score of baseline Logistic Regression model: \", train_score_lr)\n",
        "print(\"Validation ROC-AUC score of baseline Logistic Regression model: \", valid_score_lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDQS6cEvrWuk",
        "outputId": "30ce6bb3-3825-40ea-b319-7ac6569c293d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ROC-AUC score of baseline Logistic Regression model:  0.7962220697700664\n",
            "Validation ROC-AUC score of baseline Logistic Regression model:  0.787747549456662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "v1p9ONwf4BtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model \n",
        "rf = RandomForestClassifier(random_state=1)\n",
        "rf.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKYkUqVRrc4J",
        "outputId": "4a1126f7-4650-491f-8e8c-4b153e8819af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=1)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on Training and Validation datasets\n",
        "y_predict_train = rf.predict_proba(X_train_scaled)\n",
        "y_predict_valid = rf.predict_proba(X_validation_scaled)"
      ],
      "metadata": {
        "id": "0-9kzvSerc6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation with AUC \n",
        "train_score_rf = roc_auc_score(y_train, y_predict_train[:,1])\n",
        "valid_score_rf = roc_auc_score(y_validation, y_predict_valid[:,1])\n",
        "print(\"Training ROC-AUC score of Random Forest model: \", train_score_rf)\n",
        "print(\"Validation ROC-AUC score of Random Forest model: \", valid_score_rf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2UC5Oinrc8T",
        "outputId": "ad410858-941b-4564-d275-dc2f71453a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ROC-AUC score of Random Forest model:  0.9998147409217331\n",
            "Validation ROC-AUC score of Random Forest model:  0.7723518096627127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xgboost "
      ],
      "metadata": {
        "id": "Q5pNLtpc4Dm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model \n",
        "xgbc = xgb.XGBClassifier(random_state = 1, verbosity=0)\n"
      ],
      "metadata": {
        "id": "AcAXZ9q9rlzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgbc.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3uwB6mACKYu",
        "outputId": "88ca8dc3-cd95-4ef4-a0e6-83a628bc0588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(random_state=1, verbosity=0)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on Training and Validation datasets\n",
        "y_predict_train = xgbc.predict_proba(X_train_scaled)\n",
        "y_predict_valid = xgbc.predict_proba(X_validation_scaled)"
      ],
      "metadata": {
        "id": "4NrsaYxArl1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation with AUC \n",
        "train_score_xgb = roc_auc_score(y_train, y_predict_train[:,1])\n",
        "valid_score_xgb = roc_auc_score(y_validation, y_predict_valid[:,1])\n",
        "print(\"Training ROC-AUC score of XGBoost model: \", train_score_xgb)\n",
        "print(\"Validation ROC-AUC score of XGBoost model: \", valid_score_xgb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnNwI2kCrl3T",
        "outputId": "a28f1d3a-d8ec-46d8-cec8-39ca63d7a207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ROC-AUC score of XGBoost model:  0.8154911969117519\n",
            "Validation ROC-AUC score of XGBoost model:  0.7948942467124263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "q9jbWuHTfrk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear SVM"
      ],
      "metadata": {
        "id": "4gLblAn24G27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model \n",
        "lsvc = SGDClassifier(random_state = 42)\n",
        "clsvc = CalibratedClassifierCV(lsvc)\n",
        "clsvc.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd5KuB4LryvN",
        "outputId": "16657496-d3a6-4421-df29-8ab583f5800e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CalibratedClassifierCV(base_estimator=SGDClassifier(random_state=42))"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on Training and Validation datasets\n",
        "y_predict_train = clsvc.predict_proba(X_train_scaled)\n",
        "y_predict_valid = clsvc.predict_proba(X_validation_scaled)"
      ],
      "metadata": {
        "id": "dvOAEvhjryx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation with AUC \n",
        "train_score_lsvc = roc_auc_score(y_train, y_predict_train[:,1])\n",
        "valid_score_lsvc = roc_auc_score(y_validation, y_predict_valid[:,1])\n",
        "print(\"Training ROC-AUC score of Linear SVM model: \", train_score_lsvc)\n",
        "print(\"Validation ROC-AUC score of Linear SVM model: \", valid_score_lsvc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8pqU9svry0c",
        "outputId": "0ba89a79-2bab-4fe4-a9de-d31f69b861d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ROC-AUC score of Linear SVM model:  0.7702768158852363\n",
            "Validation ROC-AUC score of Linear SVM model:  0.7646037877961211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "d9PrYVeH4JRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model \n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyDz-b-ltCOj",
        "outputId": "336205af-9d4f-4c56-e1e9-9088b54c6060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB()"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on Training and Validation datasets\n",
        "y_predict_train = nb.predict_proba(X_train_scaled)\n",
        "y_predict_valid = nb.predict_proba(X_validation_scaled)"
      ],
      "metadata": {
        "id": "iY6p4Eq2tCQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation with AUC \n",
        "train_score_nb = roc_auc_score(y_train, y_predict_train[:,1])\n",
        "valid_score_nb = roc_auc_score(y_validation, y_predict_valid[:,1])\n",
        "print(\"Training ROC-AUC score of Linear SVM model: \", train_score_nb)\n",
        "print(\"Validation ROC-AUC score of Linear SVM model: \", valid_score_nb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdgAF_yZtCTE",
        "outputId": "587f55ec-a124-4eb6-b1cb-7dd89072d42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ROC-AUC score of Linear SVM model:  0.7721882272783825\n",
            "Validation ROC-AUC score of Linear SVM model:  0.7606162487723995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Model_comparison = {\n",
        "    'models':['Logistic Regression','Linear SVM', 'Naive Bayes', 'Random Forest',  'XGBoost' ],\n",
        "    'Train ROC-AUC score':[train_score_lr, train_score_lsvc, train_score_nb, train_score_rf,  train_score_xgb ],\n",
        "    'Validation ROC-AUC score':[valid_score_lr, valid_score_lsvc, valid_score_nb,valid_score_rf,  valid_score_xgb]\n",
        "}\n",
        "\n",
        "pd.DataFrame(Model_comparison)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Pu4IdwT763BB",
        "outputId": "4460463c-bb1b-4270-b7c2-54772fe36c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                models  Train ROC-AUC score  Validation ROC-AUC score\n",
              "0  Logistic Regression             0.796222                  0.787748\n",
              "1           Linear SVM             0.770277                  0.764604\n",
              "2          Naive Bayes             0.772188                  0.760616\n",
              "3        Random Forest             0.999815                  0.772352\n",
              "4              XGBoost             0.815491                  0.794894"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3281e39c-c7ff-4a46-99f2-01bedb0d0da2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>models</th>\n",
              "      <th>Train ROC-AUC score</th>\n",
              "      <th>Validation ROC-AUC score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>0.796222</td>\n",
              "      <td>0.787748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Linear SVM</td>\n",
              "      <td>0.770277</td>\n",
              "      <td>0.764604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>0.772188</td>\n",
              "      <td>0.760616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.999815</td>\n",
              "      <td>0.772352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.815491</td>\n",
              "      <td>0.794894</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3281e39c-c7ff-4a46-99f2-01bedb0d0da2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3281e39c-c7ff-4a46-99f2-01bedb0d0da2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3281e39c-c7ff-4a46-99f2-01bedb0d0da2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest is overfitting. The Bias for training data is very low but the variance is very high. We need to find a balance.\n",
        "Logistic regression seems to be more robust. Naive bayes and Linear SVM also seem okay but the score is low.\n",
        "For Naive Bayes, most of its assumptions do not hold true in current scenario. For instance the features are not independent nor Gaussian and do not contribute equally to the target prediction. Linear SVM is also performing poorly. XGBoost looks promising. "
      ],
      "metadata": {
        "id": "hXm3sCxysCn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For further analysis, I will take Logistic regression(Linear model of classification), XGBoost (Ensemble - Boosting) and Random Forest(Ensemble - Bagging) for tuning. I am curious to see if we can reduce the overfitting for Random forest. "
      ],
      "metadata": {
        "id": "Lqc3rnOk-qYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "6ach_EKrNBcR"
      }
    }
  ]
}